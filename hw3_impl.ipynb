{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "In this assignment we're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task.\n",
        "The purpose of this homework is for you to get familar with ViT and get prepared for the final project."
      ],
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xGv2wu1MyAPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04138b4b-562e-4693-9be7-dad443f2a3df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "For the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\n",
        "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ],
      "metadata": {
        "id": "MmNi93C-4rLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ],
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "      # TODO\n",
        "      assert image_size % patch_size == 0, 'Image size should be divisible by patch size'\n",
        "      super(PatchEmbedding, self).__init__()\n",
        "      #initalization of parameters\n",
        "      self.patch_size=patch_size\n",
        "      self.image_size=image_size\n",
        "      self.embed_dim=embed_dim\n",
        "      #2d convulational layer as stated\n",
        "      self.proj=nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      batch_size, _, h, w= x.shape\n",
        "      #project the input for embeddings\n",
        "      x=self.proj(x)\n",
        "      x=x.permute(0,2,3,1).view(batch_size,-1,self.embed_dim)\n",
        "      return x"
      ],
      "metadata": {
        "id": "rAzsdK5YybDa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ],
      "metadata": {
        "id": "1mk8v66y6MAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "      # TODO\n",
        "      #necessity for operations\n",
        "      assert embed_dim % num_heads == 0, \"embed_dim should be divisible by num_heads\"\n",
        "      super().__init__()\n",
        "      self.num_heads=num_heads\n",
        "      self.embed_dim=embed_dim\n",
        "      self.head_dim=embed_dim//num_heads\n",
        "      #projection as stated to capture different aspects of input\n",
        "      self.qkv_proj=nn.Linear(embed_dim, embed_dim*3)\n",
        "      #obtain outputs\n",
        "      self.out_proj=nn.Linear(embed_dim, embed_dim)\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      batch_size, seq_len, embed_dim=x.shape\n",
        "      #project the input\n",
        "      qkv=self.qkv_proj(x)\n",
        "      qkv=qkv.reshape(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
        "      qkv=qkv.permute(0,2,1,3)\n",
        "      #creating the 3 heads\n",
        "      q,k,v=qkv.chunk(3, dim=1)\n",
        "      #calculation scores\n",
        "      attn=(q @ k.transpose(-2,-1))/ math.sqrt(self.head_dim)\n",
        "      #getting the probasbilities\n",
        "      attn=F.softmax(attn,dim=-1)\n",
        "      #getting attention outputs\n",
        "      attn_out=attn @ v\n",
        "      attn_out=attn_out.transpose(1,2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "      #porjecting\n",
        "      attn_out=self.out_proj(attn_out)\n",
        "      return attn_out\n",
        "\n"
      ],
      "metadata": {
        "id": "V1LeAZq-0dQW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "You may also want to use layer normalization or other type of normalization."
      ],
      "metadata": {
        "id": "NCAURJGJ6jhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        # TODO\n",
        "        super().__init__()\n",
        "        #layer normalization as suggested along with the multi-head-self-attention\n",
        "        #we have created.\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        #The feed foward (MLP) network applys the transformations according to dimension parameters\n",
        "        #along with the dropout\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VisionTransformer:\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ],
      "metadata": {
        "id": "rgLfJRUm7EDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        # TODO\n",
        "        super(VisionTransformer,self).__init__()\n",
        "        #create the patch embeddings parameters\n",
        "        self.patch_embed=PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "        #the special token to go through layers\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        #initalize positional embeddings as self attention is used so needed\n",
        "        self.pos_embed =  nn.Parameter(torch.randn(1, (image_size // patch_size) ** 2 + 1, embed_dim))\n",
        "        #the transofrmer layers using the dimensions given\n",
        "        self.transformer = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        #layer noramloization technique used to avoid vanishing/expoding gradients\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        #head to feed for classifciation\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        batch_size=x.shape[0]\n",
        "        #feed to get patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "        #apply to each token in of batches\n",
        "        cls_token = self.cls_token.repeat(batch_size, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        #apply postional embeddings\n",
        "        x = x + self.pos_embed\n",
        "        #go through transformer layers\n",
        "        for block in self.transformer:\n",
        "            x = block(x)\n",
        "        #get special token output\n",
        "        cls_output = x[:, 0]\n",
        "        #apply to head for classification logits\n",
        "        logits = self.head(self.norm(cls_output))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
      ],
      "metadata": {
        "id": "lROdKoO37Uqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage:\n",
        "image_size =32 # TODO Keep\n",
        "patch_size = 4# TODO Keep\n",
        "in_channels = 3# TODO Keep\n",
        "embed_dim = 192 # TODO can change this and below should be divisble by both\n",
        "num_heads =12# TODO\n",
        "mlp_dim = 384 # TODO can try increasing\n",
        "num_layers = 8# TODO keep\n",
        "num_classes =100 # TODO keep\n",
        "dropout = 0.1# TODO changed from 0\n",
        "batch_size =256 # TODO changed from 128 did a little bit better"
      ],
      "metadata": {
        "id": "byAC841ix_lb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "1V14TFbM8x4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f25de7c-4f9d-4d08-d4b7-07ca65f29455"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "3BOp450mdC-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b921fd-f2bb-4d6a-8e4c-d9a8cacc7592"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)# TODO"
      ],
      "metadata": {
        "id": "4s8-X4l-exSg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100# TODO I think increasing epochs could have helped and possibly mlp_dim\n",
        "best_val_acc = 0\n",
        "scheduler = CosineAnnealingLR(optimizer,T_max=num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # TODO Feel free to modify the training loop youself.\n",
        "    scheduler.step()\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")"
      ],
      "metadata": {
        "id": "eOyk345ve5HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cadba2-77bb-4c81-cc04-047b94f1ca9f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Validation Accuracy: 15.39%\n",
            "Epoch: 2, Validation Accuracy: 21.62%\n",
            "Epoch: 3, Validation Accuracy: 22.97%\n",
            "Epoch: 4, Validation Accuracy: 26.80%\n",
            "Epoch: 5, Validation Accuracy: 29.54%\n",
            "Epoch: 6, Validation Accuracy: 31.59%\n",
            "Epoch: 7, Validation Accuracy: 33.68%\n",
            "Epoch: 8, Validation Accuracy: 34.35%\n",
            "Epoch: 9, Validation Accuracy: 36.89%\n",
            "Epoch: 10, Validation Accuracy: 38.94%\n",
            "Epoch: 11, Validation Accuracy: 41.22%\n",
            "Epoch: 12, Validation Accuracy: 42.69%\n",
            "Epoch: 13, Validation Accuracy: 42.08%\n",
            "Epoch: 14, Validation Accuracy: 44.24%\n",
            "Epoch: 15, Validation Accuracy: 45.62%\n",
            "Epoch: 16, Validation Accuracy: 46.86%\n",
            "Epoch: 17, Validation Accuracy: 46.64%\n",
            "Epoch: 18, Validation Accuracy: 47.65%\n",
            "Epoch: 19, Validation Accuracy: 48.09%\n",
            "Epoch: 20, Validation Accuracy: 48.41%\n",
            "Epoch: 21, Validation Accuracy: 48.43%\n",
            "Epoch: 22, Validation Accuracy: 49.54%\n",
            "Epoch: 23, Validation Accuracy: 49.66%\n",
            "Epoch: 24, Validation Accuracy: 51.00%\n",
            "Epoch: 25, Validation Accuracy: 50.20%\n",
            "Epoch: 26, Validation Accuracy: 51.19%\n",
            "Epoch: 27, Validation Accuracy: 50.41%\n",
            "Epoch: 28, Validation Accuracy: 51.23%\n",
            "Epoch: 29, Validation Accuracy: 51.62%\n",
            "Epoch: 30, Validation Accuracy: 51.82%\n",
            "Epoch: 31, Validation Accuracy: 52.27%\n",
            "Epoch: 32, Validation Accuracy: 52.51%\n",
            "Epoch: 33, Validation Accuracy: 51.42%\n",
            "Epoch: 34, Validation Accuracy: 52.85%\n",
            "Epoch: 35, Validation Accuracy: 52.42%\n",
            "Epoch: 36, Validation Accuracy: 52.58%\n",
            "Epoch: 37, Validation Accuracy: 52.73%\n",
            "Epoch: 38, Validation Accuracy: 53.30%\n",
            "Epoch: 39, Validation Accuracy: 52.92%\n",
            "Epoch: 40, Validation Accuracy: 53.19%\n",
            "Epoch: 41, Validation Accuracy: 52.37%\n",
            "Epoch: 42, Validation Accuracy: 53.25%\n",
            "Epoch: 43, Validation Accuracy: 52.76%\n",
            "Epoch: 44, Validation Accuracy: 53.41%\n",
            "Epoch: 45, Validation Accuracy: 53.19%\n",
            "Epoch: 46, Validation Accuracy: 53.49%\n",
            "Epoch: 47, Validation Accuracy: 53.22%\n",
            "Epoch: 48, Validation Accuracy: 53.42%\n",
            "Epoch: 49, Validation Accuracy: 53.75%\n",
            "Epoch: 50, Validation Accuracy: 53.34%\n",
            "Epoch: 51, Validation Accuracy: 53.49%\n",
            "Epoch: 52, Validation Accuracy: 54.28%\n",
            "Epoch: 53, Validation Accuracy: 54.26%\n",
            "Epoch: 54, Validation Accuracy: 53.64%\n",
            "Epoch: 55, Validation Accuracy: 53.52%\n",
            "Epoch: 56, Validation Accuracy: 53.65%\n",
            "Epoch: 57, Validation Accuracy: 53.87%\n",
            "Epoch: 58, Validation Accuracy: 54.29%\n",
            "Epoch: 59, Validation Accuracy: 54.31%\n",
            "Epoch: 60, Validation Accuracy: 54.49%\n",
            "Epoch: 61, Validation Accuracy: 54.37%\n",
            "Epoch: 62, Validation Accuracy: 54.02%\n",
            "Epoch: 63, Validation Accuracy: 54.27%\n",
            "Epoch: 64, Validation Accuracy: 53.79%\n",
            "Epoch: 65, Validation Accuracy: 54.18%\n",
            "Epoch: 66, Validation Accuracy: 54.39%\n",
            "Epoch: 67, Validation Accuracy: 55.00%\n",
            "Epoch: 68, Validation Accuracy: 54.24%\n",
            "Epoch: 69, Validation Accuracy: 54.56%\n",
            "Epoch: 70, Validation Accuracy: 54.76%\n",
            "Epoch: 71, Validation Accuracy: 54.92%\n",
            "Epoch: 72, Validation Accuracy: 54.81%\n",
            "Epoch: 73, Validation Accuracy: 54.69%\n",
            "Epoch: 74, Validation Accuracy: 54.70%\n",
            "Epoch: 75, Validation Accuracy: 55.04%\n",
            "Epoch: 76, Validation Accuracy: 54.94%\n",
            "Epoch: 77, Validation Accuracy: 55.08%\n",
            "Epoch: 78, Validation Accuracy: 55.09%\n",
            "Epoch: 79, Validation Accuracy: 54.86%\n",
            "Epoch: 80, Validation Accuracy: 55.11%\n",
            "Epoch: 81, Validation Accuracy: 55.32%\n",
            "Epoch: 82, Validation Accuracy: 55.35%\n",
            "Epoch: 83, Validation Accuracy: 55.25%\n",
            "Epoch: 84, Validation Accuracy: 55.34%\n",
            "Epoch: 85, Validation Accuracy: 55.49%\n",
            "Epoch: 86, Validation Accuracy: 55.54%\n",
            "Epoch: 87, Validation Accuracy: 55.47%\n",
            "Epoch: 88, Validation Accuracy: 55.69%\n",
            "Epoch: 89, Validation Accuracy: 55.41%\n",
            "Epoch: 90, Validation Accuracy: 55.52%\n",
            "Epoch: 91, Validation Accuracy: 55.49%\n",
            "Epoch: 92, Validation Accuracy: 55.54%\n",
            "Epoch: 93, Validation Accuracy: 55.66%\n",
            "Epoch: 94, Validation Accuracy: 55.54%\n",
            "Epoch: 95, Validation Accuracy: 55.52%\n",
            "Epoch: 96, Validation Accuracy: 55.50%\n",
            "Epoch: 97, Validation Accuracy: 55.49%\n",
            "Epoch: 98, Validation Accuracy: 55.45%\n",
            "Epoch: 99, Validation Accuracy: 55.42%\n",
            "Epoch: 100, Validation Accuracy: 55.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit your best_model.pth with this notebook. And report the best test results you get."
      ],
      "metadata": {
        "id": "-AfNVj1U9xhk"
      }
    }
  ]
}